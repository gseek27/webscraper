import scrapy
import urlparse

import csv
from datetime import datetime

data = []
import time
from time import sleep

from random import randint        

class BlueBookSpider(scrapy.Spider):
    name = "blue"
    start_urls = [
        'http://www.thebluebook.com/search.html?region=33&class=2844&searchTerm=Mechanical+Contractors&page=1',
    ]

    def parse(self, response):
        for company in response.css("div.col-sm-8.col-xs-12.search-results-col"):
            yield {
                'name': company.css('a.cname::text').extract(),
                'location': company.css('div.addy_wrapper::text').extract(),
            }
        #sleep(randint(0,10))
            
       
        pages = response.css("div.dropdown.pager-wrapper")
        length = len(''.join(BlueBookSpider.start_urls[0]))
        site = ''.join(BlueBookSpider.start_urls[0])
        url = site[0:length - 1]
        pagelist = pages.css('li a::attr(href)').extract()
        #pg = ''.join(pagelist)
        pagecount = len(pagelist)
 
        for index in range(2,8):
            page = str(url)+str(index)
            print page
            pg = pagelist[index]  
            #next_page = response.urljoin(pagelist[index])
            next_page = response.urljoin(pg)
            #print next_page
            yield scrapy.Request(next_page, callback=self.parse)
            #sleep(randint(0,10))

            #next_page = response.css('li.next a::attr(href)').extract_first()
            #if next_page is not None:
#                        next_page = response.urljoin(next_page)
#                        yield scrapy.Request(next_page, callback=self.parse)

            

# scrapy crawl blue -o blue.jl
#bash: pwd cd

# scrapy shell 'http://www.thebluebook.com/search.html?region=33&class=2844&searchTerm=Mechanical+Contractors&page=1'
#scrapyshell

# response.css("div.col-sm-8.col-xs-12.search-results-col")

# company = response.css("div.col-sm-8.col-xs-12.search-results-col")

# name = company.css("a.cname::text").extract()
# addy = company.css("div.addy_wrapper::text").extract()

#pages = response.css("div.dropdown.pager-wrapper")
#page = pages.css("a::text").extract()



# for node in response.xpath("//ul"):
#    print node.xpath("@class").extract()

# response.xpath("//a/text()").extract()
