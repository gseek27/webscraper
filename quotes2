import scrapy

from fake_useragent import UserAgent
ua = UserAgent()
header = {'User-Agent':str(ua.random)}

class NewSpider(scrapy.Spider):
    name = "quote_test"
    start_urls = ['http://quotes.toscrape.com']

    def parse(self, response):
        #SUCCESS
        #scrape the main pages for author info
        self.log('I just visited: ' + response.url)
        for q in response.css('div.quote'):
            #q = response.css('div.quote')[0]
            item = {
                'quote': q.css('span.text::text').extract_first(),
                'author': q.css('small.author::text').extract_first(),
            }
            yield item

        #follow pagination link to next page
        #SUCCESS
        next_page_url = response.css('li.next > a::attr(href)').extract_first()
        print next_page_url
        if next_page_url:
            next_page_url = response.urljoin(next_page_url)
            print next_page_url
            yield scrapy.Request(url=next_page_url, callback=self.parse)

        #SUCCESS??
        #find author links and capture url for author page
        urls = response.css('div.quote > span > a::attr(href)').extract()
        for url in urls:
            url = response.urljoin(url)
            yield scrapy.Request(url=url, callback=self.parse_details)

        #follow pagination link to next page
        #SUCCESS??
        next_page_url = response.css('li.next > a::attr(href)').extract_first()
        print next_page_url
        if next_page_url:
            next_page_url = response.urljoin(next_page_url)
            print next_page_url
            yield scrapy.Request(url=next_page_url, callback=self.parse)

    def parse_details(self, response):
        #scrape bio and descrption info from author page
        #SUCCESS??
        yield {
            'name': response.css('h3.author-title::text').extract_first(),
            'birth_date': response.css('span.author-born-date::text').extract_first(),
        }
            

#bash: pwd cd /Users/kharelthompson/Documents/Coding/python_files/pythonfiles
#bash: scrapy shell 'http://quotes.toscrape.com'
#make sure spider is in spider folder
#bash: scrapy crawl quote-test -o quote-test.jl
#bash: scrapy crawl quote-test -o quote-test.json (to see results)
#scrapy crawl spiderName -o filename.csv
#examine page structure
#scrapyshell: use response.css('element.tag')
#scrapyshell test the author block quote =response.css...

#go to proj folder with cfg
#bash: scrapy crawl quote-test
# cd /Users/kharelthompson/Documents/Coding/python_files/pythonfiles
